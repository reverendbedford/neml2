<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=11"/>
<meta name="generator" content="Doxygen 1.10.0"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>NEML2: Tensor</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<script type="text/javascript" src="clipboard.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript" src="cookie.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/javascript">
window.MathJax = {
  options: {
    ignoreHtmlClass: 'tex2jax_ignore',
    processHtmlClass: 'tex2jax_process'
  },
  loader: {
    load: ['[tex]/ams','[tex]/physics','[tex]/boldsymbol']
  },
  tex: {
    macros: {},
    packages: ['base','configmacros','ams','physics','boldsymbol']
  }
};
</script>
<script type="text/javascript" id="MathJax-script" async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="doxygen-awesome.css" rel="stylesheet" type="text/css"/>
<link href="doxygen-awesome-sidebar-only.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr id="projectrow">
  <td id="projectalign">
   <div id="projectname">NEML2<span id="projectnumber">&#160;1.4.0</span>
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.10.0 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
var searchBox = new SearchBox("searchBox", "search/",'.html');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(function() { init_search(); });
});
/* @license-end */
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function(){initNavTree('system-tensors.html',''); initResizable(); });
/* @license-end */
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<div id="MSearchResults">
<div class="SRPage">
<div id="SRIndex">
<div id="SRResults"></div>
<div class="SRStatus" id="Loading">Loading...</div>
<div class="SRStatus" id="Searching">Searching...</div>
<div class="SRStatus" id="NoMatches">No Matches</div>
</div>
</div>
</div>
</div>

<div><div class="header">
  <div class="headertitle"><div class="title">Tensor</div></div>
</div><!--header-->
<div class="contents">
<div class="toc"><h3>Table of Contents</h3>
<ul><li class="level1"><a href="#tensor-types">Tensor types</a><ul><li class="level2"><a href="#dynamically-shaped-tensor">Dynamically shaped tensor</a></li>
<li class="level2"><a href="#statically-shaped-tensor">Statically shaped tensors</a></li>
</ul>
</li>
<li class="level1"><a href="#working-with-tensors">Working with tensors</a><ul><li class="level2"><a href="#tensor-creation">Tensor creation</a></li>
<li class="level2"><a href="#tensor-broadcasting">Tensor broadcasting</a></li>
<li class="level2"><a href="#tensor-indexing">Tensor indexing</a></li>
<li class="level2"><a href="#tensor-labeling">Tensor labeling</a></li>
</ul>
</li>
</ul>
</div>
<div class="textblock"><p><a class="anchor" id="md_content_2system_2tensor"></a></p>
<p>Refer to <a class="el" href="syntax-tensors.html">Syntax Documentation</a> for the list of available objects.</p>
<h1><a class="anchor" id="tensor-types"></a>
Tensor types</h1>
<p>Currently, PyTorch is the only supported tensor backend in NEML2. Therefore, all tensor types in NEML2 directly inherit from <code>torch::Tensor</code>. In the future, support for other tensor backends may be added, but the public-facing interfaces will remain largely the same.</p>
<h2><a class="anchor" id="dynamically-shaped-tensor"></a>
Dynamically shaped tensor</h2>
<p><a class="el" href="classneml2_1_1BatchTensor.html">BatchTensor</a> is a general-purpose <em>dynamically shaped</em> tensor type for batched tensors. With a view towards vectorization, the same set of operations can be "simultaneously" applied to a "batch" of (logically the same) tensors. To provide a unified user interface for dealing with such batched operation, NEML2 assumes that the <em>first</em> \(N\) dimensions of a tensor are batched dimensions, and the following dimensions are the base (logical) dimensions.</p>
<blockquote class="doxtable">
<p>&zwj;Unlike PyTorch, NEML2 explicitly distinguishes between batch dimensions and base (logical) dimensions. </p>
</blockquote>
<p>A <code>BatchTensor</code> can be created using </p><div class="fragment"><div class="line">BatchTensor A(torch::rand({1, 1, 5, 2}), 2);</div>
</div><!-- fragment --><p> where <code>A</code> is a tensor with 2 batch dimensions. The batch sizes of <code>A</code> is <code>(1, 1)</code>: </p><div class="fragment"><div class="line"><span class="keyword">auto</span> batch_sz = A.batch_sizes();</div>
<div class="line">neml2_assert(batch_sz == {1, 1});</div>
</div><!-- fragment --><p> and the base (logical) sizes of <code>A</code> is <code>(5, 2)</code>: </p><div class="fragment"><div class="line"><span class="keyword">auto</span> base_sz = A.base_sizes();</div>
<div class="line">neml2_assert(batch_sz == {5, 2});</div>
</div><!-- fragment --><h2><a class="anchor" id="statically-shaped-tensor"></a>
Statically shaped tensors</h2>
<p><a class="el" href="classneml2_1_1FixedDimTensor.html">FixedDimTensor</a> is the parent class for all the tensor types with a <em>fixed</em> base shape. It is templated on the base shape of the tensor. NEML2 offers a rich collection of primitive tensor types inherited from <code>FixedDimTensor</code>. Currently implemented primitive tensor types are summarized below</p>
<table class="markdownTable">
<tr class="markdownTableHead">
<th class="markdownTableHeadLeft">Tensor type   </th><th class="markdownTableHeadLeft">Base shape   </th><th class="markdownTableHeadLeft">Description    </th></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyLeft"><a class="el" href="classneml2_1_1Scalar.html">Scalar</a>   </td><td class="markdownTableBodyLeft">\(()\)   </td><td class="markdownTableBodyLeft">Rank-0 tensor, i.e. scalar    </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyLeft"><a class="el" href="classneml2_1_1Vec.html">Vec</a>   </td><td class="markdownTableBodyLeft">\((3)\)   </td><td class="markdownTableBodyLeft">Rank-1 tensor, i.e. vector    </td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyLeft"><a class="el" href="classneml2_1_1R2.html">R2</a>   </td><td class="markdownTableBodyLeft">\((3,3)\)   </td><td class="markdownTableBodyLeft">Rank-2 tensor    </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyLeft"><a class="el" href="classneml2_1_1SR2.html">SR2</a>   </td><td class="markdownTableBodyLeft">\((6)\)   </td><td class="markdownTableBodyLeft">Symmetric rank-2 tensor    </td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyLeft"><a class="el" href="classneml2_1_1WR2.html">WR2</a>   </td><td class="markdownTableBodyLeft">\((3)\)   </td><td class="markdownTableBodyLeft">Skew-symmetric rank-2 tensor    </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyLeft"><a class="el" href="classneml2_1_1R3.html">R3</a>   </td><td class="markdownTableBodyLeft">\((3,3,3)\)   </td><td class="markdownTableBodyLeft">Rank-3 tensor    </td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyLeft"><a class="el" href="classneml2_1_1SFR3.html">SFR3</a>   </td><td class="markdownTableBodyLeft">\((6,3)\)   </td><td class="markdownTableBodyLeft">Rank-3 tensor with symmetry on base dimensions 0 and 1    </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyLeft"><a class="el" href="classneml2_1_1R4.html">R4</a>   </td><td class="markdownTableBodyLeft">\((3,3,3,3)\)   </td><td class="markdownTableBodyLeft">Rank-4 tensor    </td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyLeft"><a class="el" href="classneml2_1_1SSR4.html">SSR4</a>   </td><td class="markdownTableBodyLeft">\((6,6)\)   </td><td class="markdownTableBodyLeft">Rank-4 tensor with minor symmetry    </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyLeft"><a class="el" href="classneml2_1_1R5.html">R5</a>   </td><td class="markdownTableBodyLeft">\((3,3,3,3,3)\)   </td><td class="markdownTableBodyLeft">Rank-5 tensor    </td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyLeft"><a class="el" href="classneml2_1_1SSFR5.html">SSFR5</a>   </td><td class="markdownTableBodyLeft">\((6,6,3)\)   </td><td class="markdownTableBodyLeft">Rank-5 tensor with minor symmetry on base dimensions 0-3    </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyLeft"><a class="el" href="classneml2_1_1Rot.html">Rot</a>   </td><td class="markdownTableBodyLeft">\((3)\)   </td><td class="markdownTableBodyLeft">Rotation tensor represented in the Rodrigues form    </td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyLeft"><a class="el" href="classneml2_1_1Quaternion.html">Quaternion</a>   </td><td class="markdownTableBodyLeft">\((4)\)   </td><td class="markdownTableBodyLeft">Quaternion    </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyLeft"><a class="el" href="classneml2_1_1MillerIndex.html">MillerIndex</a>   </td><td class="markdownTableBodyLeft">\((3)\)   </td><td class="markdownTableBodyLeft">Crystal direction or lattice plane represented as Miller indices   </td></tr>
</table>
<p>Furthermore, all primitive tensor types can be "registered" as variables on a <code>LabeledAxis</code>, which will be discussed in the following section on <a class="el" href="#tensor-labeling">labeled view</a>.</p>
<h1><a class="anchor" id="working-with-tensors"></a>
Working with tensors</h1>
<h2><a class="anchor" id="tensor-creation"></a>
Tensor creation</h2>
<p>A factory tensor creation function produces a new tensor. All factory functions adhere to the same schema: </p><div class="fragment"><div class="line">&lt;TensorType&gt;::&lt;function_name&gt;(&lt;function-specific-options&gt;, <span class="keyword">const</span> torch::TensorOptions &amp; options);</div>
</div><!-- fragment --><p> where <code>&lt;TensorType&gt;</code> is the class name of the primitive tensor type listed above, and <code>&lt;function-name&gt;</code> is the name of the factory function which produces the new tensor. <code>&lt;function-specific-options&gt;</code> are any required or optional arguments a particular factory function accepts. Refer to each tensor type's class documentation for the concrete signature. The last argument <code>const torch::TensorOptions &amp; options</code> configures the data type, device, layout and other "meta" properties of the produced tensor. The commonly used meta properties are</p><ul>
<li><code>dtype</code>: the data type of the elements stored in the tensor. Available options are <code>kUInt8</code>, <code>kInt8</code>, <code>kInt16</code>, <code>kInt32</code>, <code>kInt64</code>, <code>kFloat32</code>, and <code>kFloat64</code>.</li>
<li><code>layout</code>: the striding of the tensor. Available options are <code>kStrided</code> (dense) and <code>kSparse</code>.</li>
<li><code>device</code>: the compute device where the tensor will be allocated. Available options are <code>kCPU</code> and <code>kCUDA</code>.</li>
<li><code>requires_grad</code>: whether the tensor is part of a function graph used by automatic differentiation to track functional relationship. Available options are <code>true</code> and <code>false</code>.</li>
</ul>
<p>For example, the following code </p><div class="fragment"><div class="line"><span class="keyword">auto</span> a = SR2::zeros({5, 3},</div>
<div class="line">                    torch::TensorOptions()</div>
<div class="line">                      .device(torch::kCPU)</div>
<div class="line">                      .layout(torch::kStrided)</div>
<div class="line">                      .dtype(torch::kFloat32));</div>
</div><!-- fragment --><p> creates a statically (base) shaped, dense, single precision tensor of type <code>SR2</code> filled with zeros, with batch shape \((5, 3)\), allocated on the CPU.</p>
<h2><a class="anchor" id="tensor-broadcasting"></a>
Tensor broadcasting</h2>
<p>Quoting Numpy's definition of broadcasting:</p>
<blockquote class="doxtable">
<p>&zwj;The term broadcasting describes how NumPy treats arrays with different shapes during arithmetic operations. Subject to certain constraints, the smaller array is “broadcast” across the larger array so that they have compatible shapes. </p>
</blockquote>
<p>NEML2's broadcasting semantics is largely the same as those of Numpy and PyTorch. However, since NEML2 explicitly distinguishes between batch and base dimensions, the broadcasting semantics must also be extended. Two NEML2 tensors are said to be <em>batch-broadcastable</em> if iterating backward from the last batch dimension, one of the following is satisfied:</p><ol type="1">
<li>Both tensors have the same size on the dimension;</li>
<li>One tensor has size 1 on the dimension;</li>
<li>The dimension does not exist in one tensor.</li>
</ol>
<p><em>Base-broadcastable</em> follows a similar definition. Most binary operators on dynamically shaped tensors, i.e., those of type <code>BatchTensor</code>, require the operands to be both batch- <em>and</em> base-broadcastable. On the other hand, most binary operators on statically base shaped tensors, i.e., those of pritimitive tensor types, only require the operands to be batch-broadcastable.</p>
<h2><a class="anchor" id="tensor-indexing"></a>
Tensor indexing</h2>
<p>In defining the forward operator of a material model, many logically different tensors representing inputs, outputs, residuals, and Jacobians have to be created, copied, and destroyed on the fly. These operations occupy a significant amount of computing time, especially on GPUs.</p>
<p>To address this challenge, NEML2 creates <em>views</em>, instead of copies, of tensors whenever possible. As its name suggests, the view of a tensor is a possibly different interpretation of the underlying data. Quoting the PyTorch documentation:</p>
<blockquote class="doxtable">
<p>&zwj;For a tensor to be viewed, the new view size must be compatible with its original size and stride, i.e., each new view dimension must either be a subspace of an original dimension, or only span across original dimensions \(d, d+1, ..., d+k\) that satisfy the following contiguity-like condition that \(\forall i = d,...,d+k-1\),  </p><p class="formulaDsp">
\[
\text{stride}[i] = \text{stride}[i+1] \times \text{size}[i+1]
\]
</p>
<p> Otherwise, it will not be possible to view self tensor as shape without copying it. </p>
</blockquote>
<p>In NEML2, use <a class="el" href="classneml2_1_1BatchTensorBase.html#a7d5d18a59ce434544c87d26d81e702f8">base_index</a> for indexing the base dimensions and <a class="el" href="classneml2_1_1BatchTensorBase.html#a7133d83ffd76e1e139b2e8d439261552">batch_index</a> for indexing the batch dimensions: </p><div class="fragment"><div class="line"><span class="keyword">using namespace </span>torch::indexing;</div>
<div class="line">BatchTensor A(torch::tensor({{2, 3, 4}, {-1, -2, 3}, {6, 9, 7}}), 1);</div>
<div class="line"><span class="comment">// A = [[  2  3  4]</span></div>
<div class="line"><span class="comment">//      [ -1 -2  3]</span></div>
<div class="line"><span class="comment">//      [  6  9  7]]</span></div>
<div class="line">BatchTensor B = A.batch_index({Slice(0, 2)});</div>
<div class="line"><span class="comment">// B = [[  2  3  4]</span></div>
<div class="line"><span class="comment">//      [ -1 -2  3]]</span></div>
<div class="line">BatchTensor C = A.base_index({Slice(1, 3)});</div>
<div class="line"><span class="comment">// C = [[  3  4]</span></div>
<div class="line"><span class="comment">//      [ -2  3]</span></div>
<div class="line"><span class="comment">//      [  9  7]]</span></div>
</div><!-- fragment --><p> To modify the content of a tensor, use <a class="el" href="classneml2_1_1BatchTensorBase.html#a9de98367492b84450c58a223e3473d4a">base_index_put</a> or <a class="el" href="classneml2_1_1BatchTensorBase.html#ada0be5f6e51644bf2c0740226b925002">batch_index_put</a>: </p><div class="fragment"><div class="line">A.base_index_put({Slice(1, 3)}, torch::ones({3, 2}));</div>
<div class="line"><span class="comment">// A = [[  2  1  1]</span></div>
<div class="line"><span class="comment">//      [ -1  1  1]</span></div>
<div class="line"><span class="comment">//      [  6  1  1]]</span></div>
<div class="line">A.batch_index_put({Slice(0, 2)}, torch::zeros({2, 3}));</div>
<div class="line"><span class="comment">// A = [[  0  0  0]</span></div>
<div class="line"><span class="comment">//      [  0  0  0]</span></div>
<div class="line"><span class="comment">//      [  6  1  1]]</span></div>
</div><!-- fragment --><p> A detailed explanation on tensor indexing APIs is available as part of the official <a href="https://pytorch.org/cppdocs/notes/tensor_indexing.html">PyTorch documentation</a>.</p>
<h2><a class="anchor" id="tensor-labeling"></a>
Tensor labeling</h2>
<p>In the context of material modeling, oftentimes views of tensors have practical/physical meanings. For example, given a logically 1D tensor with base size 9, its underlying data in an arbitrary batch may look like </p><div class="fragment"><div class="line">equivalent plastic strain   2.1</div>
<div class="line">            cauchy stress  -2.1</div>
<div class="line">                              0</div>
<div class="line">                            1.3</div>
<div class="line">                           -1.1</div>
<div class="line">                            2.5</div>
<div class="line">                            2.5</div>
<div class="line">              temperature 102.9</div>
<div class="line">                     time   3.6</div>
</div><!-- fragment --><p> where component 0 stores the scalar-valued equivalent plastic strain, components 1-6 store the tensor-valued cauchy stress (we use the Mandel notation for symmetric second order tensors), component 7 stores the scalar-valued temperature, and component 8 stores the scalar-valued time.</p>
<p>The string indicating the physical meaning of the view, e.g., "cauchy stress", is called a "label", and the view of the tensor indexed by a label is called a "labeled view", i.e., </p><div class="fragment"><div class="line">cauchy stress  -2.1</div>
<div class="line">                  0</div>
<div class="line">                1.3</div>
<div class="line">               -1.1</div>
<div class="line">                2.5</div>
<div class="line">                2.5</div>
</div><!-- fragment --><p>NEML2 provides a data structure named <a class="el" href="classneml2_1_1LabeledAxis.html">LabeledAxis</a> to facilitate the creation and modification of labels, and a data structure named <a class="el" href="classneml2_1_1LabeledTensor.html">LabeledTensor</a> to facilitate the creation and modification of labeled views.</p>
<p>The <a class="el" href="classneml2_1_1LabeledAxis.html">LabeledAxis</a> contains all information regarding how an axis of a <code>LabeledTensor</code> is labeled. The following naming convention is used:</p><ul>
<li>Item: A labelable slice of data</li>
<li>Variable: An item that is also of a <a class="el" href="#tensor-types">NEML2 primitive tensor type</a></li>
<li>Sub-axis: An item of type <code>LabeledAxis</code></li>
</ul>
<p>So yes, an axis can be labeled recursively, e.g.,</p>
<div class="fragment"><div class="line">     0 1 2 3 4 5     6     7 8 9 10 11 12   13   14</div>
<div class="line">/// |-----------| |-----| |              | |  | |  |</div>
<div class="line">///       a          b    |              | |  | |  |</div>
<div class="line">/// |-------------------| |--------------| |--| |--|</div>
<div class="line">///          sub                  a          b    c</div>
</div><!-- fragment --><p> The above example represents an axis of size 15. This axis has 4 items: <code>a</code>, <code>b</code>, <code>c</code>, and <code>sub</code>.</p><ul>
<li>"a" is a variable of storage size 6 (possibly of type <code>SR2</code>).</li>
<li>"b" is a variable of type <code>Scalar</code>.</li>
<li>"c" is a variable of type <code>Scalar</code>.</li>
<li>"sub" is a sub-axis of type <code>LabeledAxis</code>. "sub" by itself represents an axis of size 7, containing 2 items:<ul>
<li>"a" is a variable of storage size 6.</li>
<li>"b" is a variable of type <code>Scalar</code>.</li>
</ul>
</li>
</ul>
<p>Duplicate labels are <em>not</em> allowed on the same level of the axis, e.g. "a", "b", "c", and "sub" share the same level and so must be different. However, items on different levels of an axis can share the same label, e.g., "a" on the sub-axis "sub" has the same label as "a" on the main axis. In NEML2 convention, item names are always fully qualified, and a sub-axis is prefixed with a left slash, e.g. item "b" on the sub-axis "sub" can be denoted as "sub/b" on the main axis.</p>
<blockquote class="doxtable">
<p>&zwj;A label cannot contain: white spaces, quotes, left slash (<code>/</code>), or new line.</p>
<p>Due to performance considerations, a <code>LabeledAxis</code> can only be modified, e.g., adding/removing variables and sub-axis, at the time a model is constructed. After the model construction phase, the <code>LabeledAxis</code> associated with that model can no longer be modified over the entire course of the simulation. </p>
</blockquote>
<p>Refer to the documentation for a complete list of APIs for creating and modifying a <a class="el" href="classneml2_1_1LabeledAxis.html">LabeledAxis</a>.</p>
<p><a class="el" href="classneml2_1_1LabeledTensor.html">LabeledTensor</a> is the primary data structure in NEML2 for working with labeled tensor views. Each <code>LabeledTensor</code> consists of one <code>BatchTensor</code> and one or more <code>LabeledAxis</code>s. The <code>LabeledTensor</code> is templated on the base dimension \(D\). <a class="el" href="classneml2_1_1LabeledVector.html">LabeledVector</a> and <a class="el" href="classneml2_1_1LabeledMatrix.html">LabeledMatrix</a> are the two most widely used data structures in NEML2.</p>
<p><code>LabeledTensor</code> handles the creation, modification, and accessing of labeled tensors. Recall that all primitive data types in a labeled tensor are flattened, e.g., a symmetric fourth order tensor of type <code>SSR4</code> with batch size <code>(5)</code> and base size <code>(6, 6)</code> are flattened to have base size <code>(36)</code> in the labeled tensor. The documentation provides a complete list of APIs. The commonly used methods are</p><ul>
<li><a class="el" href="classneml2_1_1LabeledTensor.html#a2e2fb92a0c6d84ad7d6cbb2c00de011e">operator()</a> for retrieving a labeled view into the raw (flattened) data without reshaping</li>
<li><a class="el" href="classneml2_1_1LabeledTensor.html#afec36430e0f802eeccaf877e68d6f553">get</a> for retrieving a labeled view and reshaping it to the correct shape</li>
<li><a class="el" href="classneml2_1_1LabeledTensor.html#aaadfa72fc5b16ef9a08aebcca86411d9">set</a> for setting values for a labeled view</li>
<li><a class="el" href="classneml2_1_1LabeledTensor.html#a25c6a6bc3b0d8973ccbf8bca512d33e0">slice</a> for slicing a sub-axis along a specific base dimension</li>
<li><a class="el" href="classneml2_1_1LabeledTensor.html#a73978d4287506808ddd5adea60eda28f">block</a> for sub-indexing the <code>LabeledTensor</code> with \(D\) sub-axis names </li>
</ul>
</div></div><!-- contents -->
</div><!-- PageDoc -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="footer">Generated by <a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.10.0 </li>
  </ul>
</div>
</body>
</html>
